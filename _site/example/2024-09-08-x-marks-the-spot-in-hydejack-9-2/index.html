<p class="lead">Everything You Need to Know about Time Series (Key Points)</p>

<p><a href="#linking-in-style">Modernized</a> <a href="#whats-in-the-cards">design</a>, <a href="#ready-for-the-big-screen">big headlines</a>, big new features: <a href="#built-in-search">Built-In Search</a>, <a href="#sticky-table-of-contents">Sticky Table of Contents</a>, and <a href="#auto-hiding-navbar">Auto-Hiding Navbar</a>. That <a href="#and-much-more">and more</a> is Hydejack 9.</p>

<h1 id="everything-you-need-to-know-about-time-series"><strong>Everything You Need to Know about Time Series</strong></h1>

<h2 id="introduction"><strong>Introduction</strong></h2>

<p>This guide severs as a comprehensive container for various key topics in time series analysis. It highlights only the essential concepts and headings you need to be familiar with before addressing a time series problem. If you’re unsure about any topic, you can easily google it and look it up for further details.</p>

<h2 id="definition"><strong>Definition</strong></h2>

<p>Time Series is a series of observations taken at specified time intervals usually equal intervals.</p>

<h2 id="resampling"><strong>Resampling</strong></h2>

<ul>
  <li>Upsampling - Time series is resampled from low frequency to high frequency (Monthly to daily frequency). It involves filling or interpolating missing data</li>
  <li>Downsampling - Time series is resampled from high frequency to low frequency (Weekly to monthly frequency). It involves aggregation of existing data.</li>
</ul>

<h2 id="handling-missing-values"><strong>Handling Missing Values</strong></h2>

<ul>
  <li>Fill NaN with Mean Value</li>
  <li>Fill NaN with Last Value with .ffill()</li>
  <li>Fill NaN with Linearly Interpolated Value</li>
  <li>Seasonal Interpolation: For data with seasonality, use seasonal patterns to fill in gaps.</li>
  <li>Imputation with models: Use time series models (e.g., ARIMA, Kalman filters) to predict missing values based on surrounding data.</li>
</ul>

<h2 id="identifying-outliers"><strong>Identifying outliers</strong></h2>

<ul>
  <li>Statistical techniques like Z-scores.</li>
  <li>Other techniques: see the last section of the guide.</li>
</ul>

<h2 id="comparing-two-time-series"><strong>Comparing two time series</strong></h2>

<p>To compare two time series by normalizing them such that they both start with the same value. Dividing them with the initial value and then multiply it by a reference value.</p>

<h2 id="autocorrelation"><strong>Autocorrelation</strong></h2>

<p>The autocorrelation function (ACF) measures how a series is correlated with itself at different lags. Lag: The delay between a value and its previous value(s). Time series models like ARIMA depend on choosing the correct number of lags. Plotting ACF helps identify appropriate lags. If a series is significantly autocorrelated, that means, the previous values of the series (lags) may be helpful in predicting the current value.</p>

<h2 id="partial-autocorrelation"><strong>Partial Autocorrelation</strong></h2>

<p>The partial autocorrelation function can be interpreted as a regression of the series against its past lags. The terms can be interpreted the same way as a standard linear regression, that is the contribution of a change in that particular lag while holding others constant.</p>

<h2 id="components-of-a-time-series"><strong>Components of a time series</strong></h2>

<ul>
  <li>Trend - Consistent upwards or downwards slope of a time series</li>
  <li>Seasonality - Clear periodic pattern of a time series (like sine function)</li>
  <li>Noise - Outliers or missing values. White noise has Constant mean, variance, and zero auto-correlation at all lags</li>
  <li>Cyclicity - behavior that repeats itself after large interval of time, like months, years etc. It happens when the rise and fall pattern in the series does not happen in fixed calendar-based intervals.</li>
</ul>

<h2 id="modeling-time-series"><strong>Modeling time series</strong></h2>

<ul>
  <li>Additive time series: Value = Base Level + Trend + Seasonality + Error</li>
  <li>Multiplicative Time Series: Value = Base Level x Trend x Seasonality x Error
If seasonality tends to be constant over time, the additive model is suggestive. If the seasonality seems to increase with time then the multiplicative model is suggestive.</li>
</ul>

<h2 id="random-walk"><strong>Random walk</strong></h2>

<p>A random walk is a mathematical object, known as a stochastic or random process, that describes a path that consists of a succession of random steps on some mathematical space such as the integers.
Pt = Pt-1 + εt. Example: Today’s Price = Yesterday’s Price + Noise
Random walks can’t be forecasted because well, noise is random.</p>

<h2 id="stationary"><strong>Stationary</strong></h2>

<p>A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. So, the values are independent of time. Stationarity is important as non-stationary series that depend on time have too many parameters to account for when modelling the time series.</p>

<h2 id="tests-to-check-if-a-time-serie-is-stationary-or-not"><strong>Tests to check if a time serie is stationary or not</strong></h2>

<p>Rolling Statistics - Plot the moving avg or moving standard deviation to see if it varies with time. It’s a visual technique.
ADCF Test - Augmented Dickey–Fuller test is used to gives us various values (Test Statistics &amp; some critical values for some confidence levels) that can help in identifying stationarity. The Null hypothesis says that a TS is non-stationary. The Null Hypothesis to be rejected and accepting that the time series is stationary, there are 2 requirements:</p>

<ol>
  <li>Critical Value (5%) &gt; Test Statistic</li>
  <li>p-value &lt; 0.05</li>
</ol>

<h2 id="transformations-from-non--stationary-to-stationary"><strong>Transformations from non- stationary to stationary</strong></h2>

<ul>
  <li>Differencing the Series (once or more): If the autocorrelations are positive for many numbers of lags (10 or more), then the series needs further differencing. On the other hand, if the lag 1 autocorrelation itself is too negative, then the series is probably over-differenced.</li>
  <li>Take the log of the series</li>
  <li>Take the nth root of the series</li>
  <li>Combination of the above</li>
  <li>Exponential decay</li>
</ul>

<h2 id="granger-causality-test"><strong>Granger causality test</strong></h2>

<p>It is used to determine if one time series will be useful to forecast another.</p>

<h2 id="facebook-prophet"><strong>Facebook Prophet</strong></h2>

<p>A tool for forecasting time series with trends and seasonality, particularly for irregular data.</p>

<h2 id="arima"><strong>ARIMA</strong></h2>

<p>ARIMA (Auto Regressive Integrated Moving Average) is a combination of 2 models AR(Auto Regressive) &amp; MA(Moving Average). It has 3 hyperparameters: P(auto regressive lags),d(order of differentiation),Q(moving avg.). The AR part is correlation between prev &amp; current time periods. AR is a model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations. The I part binds together the AR &amp; MA parts. Basically, Today’s return = mean + Yesterday’s return + noise + yesterday’s noise. It does not support seasonality.</p>

<h2 id="sarima"><strong>SARIMA</strong></h2>

<p>SARIMA models are useful for modeling seasonal time series, in which the mean and other statistics for a given season are not stationary across the years. The SARIMA model defined constitutes a straightforward extension of the nonseasonal autoregressive-moving average (ARMA) and autoregressive integrated moving average (ARIMA) models presented.</p>

<h2 id="find-value-of-p--q-for-arima"><strong>Find value of P &amp; Q for ARIMA</strong></h2>

<p>We need to take help of ACF(Auto Correlation Function) &amp; PACF(Partial Auto Correlation Function) plots. ACF &amp; PACF graphs are used to find value of P &amp; Q for ARIMA. We need to check, for which value in x-axis, graph line drops to 0 in y-axis for 1st time.
From PACF(at y=0), get P
From ACF(at y=0), get Q</p>

<h2 id="other-models"><strong>Other Models</strong></h2>

<ul>
  <li>Transformers with time series: https://medium.com/intel-tech/how-to-apply-transformers-to-time-series-models-spacetimeformer-e452f2825d2e</li>
  <li>Regression after extracting features from the date:</li>
</ul>

<ol>
  <li>Rolling Statistics: Calculate rolling mean, median, or standard deviation over a moving window to capture trends and variations.</li>
  <li>Lagged Variables: Create lagged versions of your time series to capture historical patterns and correlations.</li>
  <li>Fourier Transforms: Convert time-domain data into frequency-domain data using Fourier transforms to identify periodic patterns.</li>
  <li>‘day_of_week’, ‘month’, ‘quarter’, ‘year’…</li>
  <li>LSTM</li>
</ol>

<h2 id="accuracy-metrics"><strong>Accuracy metrics</strong></h2>

<ul>
  <li>Mean Absolute Percentage Error (MAPE)</li>
  <li>Mean Error (ME)</li>
  <li>Mean Absolute Error (MAE)</li>
  <li>Mean Percentage Error (MPE)</li>
  <li>Root Mean Squared Error (RMSE)</li>
</ul>

<h2 id="univariate-vs-multivariate"><strong>Univariate vs Multivariate</strong></h2>

<p>Univariate Time Series: Models are trained on a single variable</p>

<p>Multivariate Time Series: Involves multiple variables. The model learns from the interactions between these variables to improve predictions. Capture both autocorrelations and inter-variable correlations. Models to be used: VAR (Vector Autoregressive Models), LSTM, Transformers models. Multi variate time series, a special Time series approach:</p>

<ol>
  <li>The idea is to use a Keras Conv2D (usually used for image analysis) on this time series</li>
  <li>Prep the data as chunks or buckets of x time steps and y features each</li>
  <li>Exposing a Conv2D to the above prepped data, which should hopefully make the algorithm task easier</li>
</ol>

<h2 id="use-case-anomalies-detection-in-time-series"><strong>Use Case: Anomalies Detection in time series</strong></h2>

<p><img src="/assets/img/blog/timeseriesanomalies.png" alt="drawing" width="800" /></p>

<p>Anomalies are data points that deviate significantly from the underlying pattern of the time series. These deviations can be caused by various factors such as sudden events, errors in data collection, or changes in the underlying process.</p>

<p>Moving average and exponential smoothing are techniques used to smooth out noise and fluctuations in time series data. Anomalies can be detected by comparing the observed values to the smoothed values. Sudden deviations between the two may indicate the presence of anomalies. These methods are especially useful for detecting anomalies in data with seasonal patterns.</p>

<p>Anomalies can be identified by comparing the predicted values of a model like ARIMA OR LSTM with the actual values. Significant differences may indicate the presence of anomalies.</p>

<p>We can also use classification after extracting features from the date.</p>

<p>In addition, Anomaly detection in time series data may be accomplished using unsupervised learning approaches like clustering, PCA (Principal Component Analysis), and autoencoders. The autoencoder is an unsupervised neural network that learns to reconstruct its input data by first compressing input data into a lower-dimensional representation and then extending it back to its original dimensions. An autoencoder may be trained on typical time series data to learn a compressed version of the data for anomaly identification. The anomaly score may then be calculated using the reconstruction error between the original and reconstructed data.</p>
