<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-09-28T23:10:26+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">SAMAHA Pierre</title><subtitle>Data Scientist with a robut background, including a Master&apos;s Degree in Engineering from Mines Paris (Diplôme d&apos;Ingénieur) and a proven track record at IBM across diverse domains, including industry, finance, real estate, energy, public work and e-commerce. Currently developing Clara AI, an assistant data scientist designed to streamline code writing and enhance AI-driven solutions. Passionate about advancing the AI field, I focus on creating innovative tools that help businesses harness the full potential of AI for growth and efficiency. My aim is to push the boundaries of AI, automating processes and unlocking new opportunities for organizations Based in Paris
</subtitle><author><name>Pierre Samaha</name><email>pierre_samaha@hotmail.com</email></author><entry><title type="html">CLARA AI My Data Scientist Assistant</title><link href="http://localhost:4000/example/2024-09-01-CLARA-AI-My-Data-Scientist-Assistant/" rel="alternate" type="text/html" title="CLARA AI My Data Scientist Assistant" /><published>2024-09-01T00:00:00+02:00</published><updated>2024-09-28T19:12:17+02:00</updated><id>http://localhost:4000/example/CLARA-AI-My-Data-Scientist-Assistant</id><content type="html" xml:base="http://localhost:4000/example/2024-09-01-CLARA-AI-My-Data-Scientist-Assistant/"><![CDATA[<p>CLARA AI is a personal project I built for myself to help me build machine learning code for any task. I have meticulously created each step of the machine learning pipeline so that I can generate code easily thanks to a step-by-step guided tool with all the information in a single interface. I can ask CLARA AI any other question with the built-in chat using LLM generative AI and then add the answer to the code.</p>

<h2 id="you-are-in-control">You are in Control</h2>

<p>The Only AutoML with Full Control of Your Code:
CLARA AI is designed to save time for a data scientist. It is no black box, as you can see exactly how the ML pipeline is constructed and you will build it by yourself with a few clicks. No dependencies on many disparate libraries that increase complexity, maintenance costs, and technical debt in your projects.</p>

<h2 id="what-you-get-with-clara-ai">What you get with CLARA AI?</h2>

<ul>
  <li>
    <p><strong>Dynamic Python Code Generation:</strong> Generation of python machine learning code in real-time, providing instant visibility into the code being created.</p>
  </li>
  <li><strong>Guided Step-by-Step Process:</strong> Our platform offers clear, step-by-step guidance tailored to both data scientists and newcomers, ensuring a deeper understanding of algorithmic workings.</li>
  <li><strong>Comprehensible Code:</strong> The generated code is readily comprehensible to any data scientist, enabling easy modification by your team without reliance on additional libraries. This puts the power back in your hands.</li>
  <li><strong>Expert Data Scientist Collaboration:</strong> Benefit from the experience of a team of data scientists who have meticulously crafted each step available in the application, powered by AI-driven insights.</li>
  <li><strong>Fast development:</strong> Guided by generative AI, the machine learning technology with code generated that works in terms of hours and not days or weeks.</li>
  <li><strong>Customizable Code:</strong> In contrast to DraftifAI, other AutoML tools lack customization to specific business needs, requiring domain knowledge for method selection in imputation of missing data or feature extraction.</li>
</ul>

<h2 id="how-it-works">How it works?</h2>

<p><img src="/assets/img/blog/claraai.gif" alt="Gif" class="lead" width="1500" height="600" /></p>

<p><a href="https://x.com/ClaraJuniorDS/status/1838533934226821583">Full link with explanations</a></p>

<p>In this demo, we choose a regression task. First, we upload the data to be read and analyzed, then specify the target variable we want to predict. Clara AI generates the corresponding Python machine learning code in real-time. It also provides fully interactive visualizations to aid in data analysis.</p>

<p>Next, we proceed with preprocessing the data, such as feature extraction and data cleaning. Clara guides users step-by-step through each data preprocessing task, ensuring a deeper understanding of the underlying algorithms.</p>

<p>We then split the data into training and test sets and move to the model selection phase. Clara instantly generates personalized code for this step as well.</p>

<p>Additionally, users can ask Clara to generate code using a large language model (LLM). After receiving the response, we can integrate the generated code directly into the notebook.</p>

<p>Finally, after selecting the best model, users can either download the generated code or obtain the trained model directly.</p>

<h2 id="better-data-scientists">Better DATA Scientists</h2>

<p>AI won’t take the job of a data scientist. Data scientists that use AI wisely work better and faster. They must always be in control of what they do. This is how CLARA AI differs from anypother autoML tool. Jobs are changing not disappearing. We need to improve our job and add to them. Instead of fearing that AI will eliminate jobs, let’s see how it helps us, as data scientist or not, to create new and better ones. CLARA AI helpes me with my work to create faster and complete code with no missing steps.</p>]]></content><author><name>Pierre Samaha</name><email>pierre_samaha@hotmail.com</email></author><category term="example" /><summary type="html"><![CDATA[Introducing Clara AI: The AI data scientist designed to help you generate ML code in hours, not weeks!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/clarablog.png" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/clarablog.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Everything You Need To Know About Time Series</title><link href="http://localhost:4000/example/2024-08-08-Everything-You-Need-to-Know-about-Time-Series/" rel="alternate" type="text/html" title="Everything You Need To Know About Time Series" /><published>2024-08-08T00:00:00+02:00</published><updated>2024-09-28T16:13:55+02:00</updated><id>http://localhost:4000/example/Everything-You-Need-to-Know-about-Time-Series</id><content type="html" xml:base="http://localhost:4000/example/2024-08-08-Everything-You-Need-to-Know-about-Time-Series/"><![CDATA[<!--more-->

<h2 id="introduction">Introduction</h2>

<p>This guide severs as a comprehensive container for various key topics in time series analysis. It highlights only the essential concepts and headings you need to be familiar with before addressing a time series problem. If you’re unsure about any topic, you can easily google it and look it up for further details.</p>

<h2 id="definition">Definition</h2>

<p>Time Series is a series of observations taken at specified time intervals usually equal intervals.</p>

<h2 id="resampling">Resampling</h2>

<ul>
  <li>Upsampling - Time series is resampled from low frequency to high frequency (Monthly to daily frequency). It involves filling or interpolating missing data</li>
  <li>Downsampling - Time series is resampled from high frequency to low frequency (Weekly to monthly frequency). It involves aggregation of existing data.</li>
</ul>

<h2 id="handling-missing-values"><strong>Handling Missing Values</strong></h2>

<ul>
  <li>Fill NaN with Mean Value</li>
  <li>Fill NaN with Last Value with .ffill()</li>
  <li>Fill NaN with Linearly Interpolated Value</li>
  <li>Seasonal Interpolation: For data with seasonality, use seasonal patterns to fill in gaps.</li>
  <li>Imputation with models: Use time series models (e.g., ARIMA, Kalman filters) to predict missing values based on surrounding data.</li>
</ul>

<h2 id="identifying-outliers"><strong>Identifying outliers</strong></h2>

<ul>
  <li>Statistical techniques like Z-scores.</li>
  <li>Other techniques: see the last section of the guide.</li>
</ul>

<h2 id="comparing-two-time-series"><strong>Comparing two time series</strong></h2>

<p>To compare two time series by normalizing them such that they both start with the same value. Dividing them with the initial value and then multiply it by a reference value.</p>

<h2 id="autocorrelation"><strong>Autocorrelation</strong></h2>

<p>The autocorrelation function (ACF) measures how a series is correlated with itself at different lags. Lag: The delay between a value and its previous value(s). Time series models like ARIMA depend on choosing the correct number of lags. Plotting ACF helps identify appropriate lags. If a series is significantly autocorrelated, that means, the previous values of the series (lags) may be helpful in predicting the current value.</p>

<h2 id="partial-autocorrelation"><strong>Partial Autocorrelation</strong></h2>

<p>The partial autocorrelation function can be interpreted as a regression of the series against its past lags. The terms can be interpreted the same way as a standard linear regression, that is the contribution of a change in that particular lag while holding others constant.</p>

<h2 id="components-of-a-time-series"><strong>Components of a time series</strong></h2>

<ul>
  <li>Trend - Consistent upwards or downwards slope of a time series</li>
  <li>Seasonality - Clear periodic pattern of a time series (like sine function)</li>
  <li>Noise - Outliers or missing values. White noise has Constant mean, variance, and zero auto-correlation at all lags</li>
  <li>Cyclicity - behavior that repeats itself after large interval of time, like months, years etc. It happens when the rise and fall pattern in the series does not happen in fixed calendar-based intervals.</li>
</ul>

<h2 id="modeling-time-series"><strong>Modeling time series</strong></h2>

<ul>
  <li>Additive time series: Value = Base Level + Trend + Seasonality + Error</li>
  <li>Multiplicative Time Series: Value = Base Level x Trend x Seasonality x Error
If seasonality tends to be constant over time, the additive model is suggestive. If the seasonality seems to increase with time then the multiplicative model is suggestive.</li>
</ul>

<h2 id="random-walk"><strong>Random walk</strong></h2>

<p>A random walk is a mathematical object, known as a stochastic or random process, that describes a path that consists of a succession of random steps on some mathematical space such as the integers.
Pt = Pt-1 + εt. Example: Today’s Price = Yesterday’s Price + Noise
Random walks can’t be forecasted because well, noise is random.</p>

<h2 id="stationary"><strong>Stationary</strong></h2>

<p>A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. So, the values are independent of time. Stationarity is important as non-stationary series that depend on time have too many parameters to account for when modelling the time series.</p>

<h2 id="tests-to-check-if-a-time-serie-is-stationary-or-not"><strong>Tests to check if a time serie is stationary or not</strong></h2>

<p>Rolling Statistics - Plot the moving avg or moving standard deviation to see if it varies with time. It’s a visual technique.
ADCF Test - Augmented Dickey–Fuller test is used to gives us various values (Test Statistics &amp; some critical values for some confidence levels) that can help in identifying stationarity. The Null hypothesis says that a TS is non-stationary. The Null Hypothesis to be rejected and accepting that the time series is stationary, there are 2 requirements:</p>

<ol>
  <li>Critical Value (5%) &gt; Test Statistic</li>
  <li>p-value &lt; 0.05</li>
</ol>

<h2 id="transformations-from-non--stationary-to-stationary"><strong>Transformations from non- stationary to stationary</strong></h2>

<ul>
  <li>Differencing the Series (once or more): If the autocorrelations are positive for many numbers of lags (10 or more), then the series needs further differencing. On the other hand, if the lag 1 autocorrelation itself is too negative, then the series is probably over-differenced.</li>
  <li>Take the log of the series</li>
  <li>Take the nth root of the series</li>
  <li>Combination of the above</li>
  <li>Exponential decay</li>
</ul>

<h2 id="granger-causality-test"><strong>Granger causality test</strong></h2>

<p>It is used to determine if one time series will be useful to forecast another.</p>

<h2 id="facebook-prophet"><strong>Facebook Prophet</strong></h2>

<p>A tool for forecasting time series with trends and seasonality, particularly for irregular data.</p>

<h2 id="arima"><strong>ARIMA</strong></h2>

<p>ARIMA (Auto Regressive Integrated Moving Average) is a combination of 2 models AR(Auto Regressive) &amp; MA(Moving Average). It has 3 hyperparameters: P(auto regressive lags),d(order of differentiation),Q(moving avg.). The AR part is correlation between prev &amp; current time periods. AR is a model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations. The I part binds together the AR &amp; MA parts. Basically, Today’s return = mean + Yesterday’s return + noise + yesterday’s noise. It does not support seasonality.</p>

<h2 id="sarima"><strong>SARIMA</strong></h2>

<p>SARIMA models are useful for modeling seasonal time series, in which the mean and other statistics for a given season are not stationary across the years. The SARIMA model defined constitutes a straightforward extension of the nonseasonal autoregressive-moving average (ARMA) and autoregressive integrated moving average (ARIMA) models presented.</p>

<h2 id="find-value-of-p--q-for-arima"><strong>Find value of P &amp; Q for ARIMA</strong></h2>

<p>We need to take help of ACF(Auto Correlation Function) &amp; PACF(Partial Auto Correlation Function) plots. ACF &amp; PACF graphs are used to find value of P &amp; Q for ARIMA. We need to check, for which value in x-axis, graph line drops to 0 in y-axis for 1st time.
From PACF(at y=0), get P
From ACF(at y=0), get Q</p>

<h2 id="other-models"><strong>Other Models</strong></h2>

<ul>
  <li>Transformers with time series: <a href="https://medium.com/intel-tech/how-to-apply-transformers-to-time-series-models-spacetimeformer-e452f2825d2e">Link for Transformers with Time Series</a></li>
  <li>Regression after extracting features from the date:</li>
</ul>

<ol>
  <li>Rolling Statistics: Calculate rolling mean, median, or standard deviation over a moving window to capture trends and variations.</li>
  <li>Lagged Variables: Create lagged versions of your time series to capture historical patterns and correlations.</li>
  <li>Fourier Transforms: Convert time-domain data into frequency-domain data using Fourier transforms to identify periodic patterns.</li>
  <li>‘day_of_week’, ‘month’, ‘quarter’, ‘year’…</li>
  <li>LSTM</li>
</ol>

<h2 id="accuracy-metrics"><strong>Accuracy metrics</strong></h2>

<ul>
  <li>Mean Absolute Percentage Error (MAPE)</li>
  <li>Mean Error (ME)</li>
  <li>Mean Absolute Error (MAE)</li>
  <li>Mean Percentage Error (MPE)</li>
  <li>Root Mean Squared Error (RMSE)</li>
</ul>

<h2 id="univariate-vs-multivariate"><strong>Univariate vs Multivariate</strong></h2>

<p>Univariate Time Series: Models are trained on a single variable</p>

<p>Multivariate Time Series: Involves multiple variables. The model learns from the interactions between these variables to improve predictions. Capture both autocorrelations and inter-variable correlations. Models to be used: VAR (Vector Autoregressive Models), LSTM, Transformers models. Multi variate time series, a special Time series approach:</p>

<ol>
  <li>The idea is to use a Keras Conv2D (usually used for image analysis) on this time series</li>
  <li>Prep the data as chunks or buckets of x time steps and y features each</li>
  <li>Exposing a Conv2D to the above prepped data, which should hopefully make the algorithm task easier</li>
</ol>

<h2 id="use-case-anomalies-detection-in-time-series"><strong>Use Case: Anomalies Detection in time series</strong></h2>

<p><img src="/assets/img/blog/timeseriesanomalies.png" alt="drawing" width="800" /></p>

<p>Anomalies are data points that deviate significantly from the underlying pattern of the time series. These deviations can be caused by various factors such as sudden events, errors in data collection, or changes in the underlying process.</p>

<p>Moving average and exponential smoothing are techniques used to smooth out noise and fluctuations in time series data. Anomalies can be detected by comparing the observed values to the smoothed values. Sudden deviations between the two may indicate the presence of anomalies. These methods are especially useful for detecting anomalies in data with seasonal patterns.</p>

<p>Anomalies can be identified by comparing the predicted values of a model like ARIMA OR LSTM with the actual values. Significant differences may indicate the presence of anomalies.</p>

<p>We can also use classification after extracting features from the date.</p>

<p>In addition, Anomaly detection in time series data may be accomplished using unsupervised learning approaches like clustering, PCA (Principal Component Analysis), and autoencoders. The autoencoder is an unsupervised neural network that learns to reconstruct its input data by first compressing input data into a lower-dimensional representation and then extending it back to its original dimensions. An autoencoder may be trained on typical time series data to learn a compressed version of the data for anomaly identification. The anomaly score may then be calculated using the reconstruction error between the original and reconstructed data.</p>]]></content><author><name>Pierre Samaha</name><email>pierre_samaha@hotmail.com</email></author><category term="example" /><summary type="html"><![CDATA[This guide severs as a comprehensive container for various key topics in time series analysis. It highlights only the essential concepts and headings you need to be familiar with before addressing a time series problem. If you're unsure about any topic, you can easily google it and look it up for further details.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/timeseries.jpg" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/timeseries.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">New Classification Method you Probably Haven’t Heard of</title><link href="http://localhost:4000/example/2024-08-01-New-Classification-Method-you-Probably-Haven't-Heard-of/" rel="alternate" type="text/html" title="New Classification Method you Probably Haven’t Heard of" /><published>2024-08-01T00:00:00+02:00</published><updated>2024-09-28T23:07:36+02:00</updated><id>http://localhost:4000/example/New-Classification-Method-you-Probably-Haven&apos;t-Heard-of</id><content type="html" xml:base="http://localhost:4000/example/2024-08-01-New-Classification-Method-you-Probably-Haven&apos;t-Heard-of/"><![CDATA[<p>You’ve probably heard of many classification models for tabular data, from SVMs and logistic regression, to decision tree-based models such as Bagging or Boosting classifiers, to stacking models… This guide will introduce you to many other unconventional approaches, which you’ve probably never heard of, and which may offer new perspectives or lead to unexpected performance gains.</p>

<p>Deep learning trend from Google Trend</p>

<p><img src="/assets/img/blog/deeplearningtrend.jpg" alt="drawing" width="500" /></p>

<p>The representation of tabular data in image form via radar diagrams is a novelty or another method that remains to be explored and researched. The Vision Transformer <a href="https://huggingface.co/docs/transformers/model_doc/vit">ViT</a> (ViT) is already used for image classification, so the aim is to see whether transforming tabular data into images can add value.</p>

<p><img src="/assets/img/blog/vit_architecture.jpg" alt="drawing" width="500" /></p>

<p>Radar plots (also known as spider plots or radar charts) are a popular data visualisation tool that allows us to compare datasets by displaying multiple variables simultaneously on a 2-dimensional plot. Radar charts provide an excellent way to visualize one or more groups of values (depends if it is a binary classification or not) over multiple variables (features of the dataset) (see the image below).</p>

<p><img src="/assets/img/blog/radarimage.jpg" alt="drawing" width="400" /></p>

<p>This could be an example of a method for transforming tabular data into images. Other methods involve representing each pixel of the image by the normalized value of the feature.</p>

<h2 id="strengths-of-the-approach-vit-on-radar-diagrams">Strengths of the approach (ViT on radar diagrams)</h2>

<ol>
  <li>Capture complex information: Radar diagrams can highlight relationships between features visually, and ViT can capture these relationships through self-attention. This makes it possible to model global dependencies and uncover complex patterns that conventional methods might miss. This approach can be particularly useful if the relationships between features are non-linear or complex, but is probably less suited to data with independent features.</li>
</ol>

<h2 id="potential-challenges-of-the-approach">Potential challenges of the approach</h2>

<ol>
  <li>
    <p>Loss of information: Transforming numerical data into 2D graphs could lead to a loss of information, whereas classical models better preserve the details of the raw data.</p>
  </li>
  <li>
    <p>Complexity, resources and interpretability: Training a ViT requires more resources than classical models, which can offer similar results with less complexity and may be easier to interpret (especially for use cases such as telecom churn).</p>
  </li>
  <li>
    <p>Data efficiency : ViTs often require large data sets, whereas classical models are more efficient on smaller sets.</p>
  </li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>This approach, along with other DNN techniques for tabular data (such as image transformation or the use of transformers without image transformation), can be useful in some cases, but not always. In particular, it is slower in prediction, which can pose problems in solutions where speed is essential.</p>

<p>Experimentation with different techniques and rigorous evaluation are always essential to identify the best approach for each specific use case.</p>

<p>Future research could focus on optimizing image generation techniques (such as bar charts, for example) and exploring hybrid models that combine the strengths of transformers and traditional approaches. See this article about converting tabular data into prompts with LLMs that produce contextual embeddings for each row, enabling it to identify subtle correlations in the data. For churn prediction, these embeddings are then tabulated and integrated into a CatBoost model. <a href="https://www.irjmets.com/uploadedfiles/paper/issue_5_may_2024/57236/final/fin_irjmets1718378031.pdf">LLM for classification of tabular data</a></p>

<p>Different Approches (unconventional approches) for classying tabular data:</p>

<p><img src="/assets/img/blog/optionsclassification.png" alt="drawing" /></p>

<p>Links for reashers focusing on this field:</p>

<p><a href="https://www.nature.com/articles/s41598-021-90923-y">Link1</a></p>

<p><a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/9d86d83f925f2149e9edb0ac3b49229c-Paper.pdf">Link2</a></p>

<p><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0295598">Link3</a></p>

<p><a href="https://www.sciencedirect.com/science/article/pii/S0167739X24003510">Link4</a></p>

<p><a href="https://arxiv.org/pdf/2401.15238">Link5</a></p>]]></content><author><name>Pierre Samaha</name><email>pierre_samaha@hotmail.com</email></author><category term="example" /><summary type="html"><![CDATA[New Approach to Classification Based on Transformers, LLM, and Embedding]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/optionsclassification.png" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/optionsclassification.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Sustainability and GenAI</title><link href="http://localhost:4000/example/2024-03-01-Sustainability-and-GenAI/" rel="alternate" type="text/html" title="Sustainability and GenAI" /><published>2024-03-01T00:00:00+01:00</published><updated>2024-09-28T20:53:52+02:00</updated><id>http://localhost:4000/example/Sustainability-and-GenAI</id><content type="html" xml:base="http://localhost:4000/example/2024-03-01-Sustainability-and-GenAI/"><![CDATA[<p>Measuring the environmental impact of digital: Why estimate carbon emissions (quantity of CO₂e emissions)?</p>

<ul>
  <li>Refine practices by using figures to determine whether an investment in a project is justified.</li>
  <li>Provide figures to CSR departments responsible for producing an annual CSR report.</li>
  <li>Objectively assess the impact of IT to facilitate dialogue on the subject.
Or simply because, as Niels Bohr put it, “Nothing exists until it is measured”.</li>
</ul>

<p>These reasons are relevant to every digital project. Assessing CO2 emissions for GENAI projects raises questions such as: how much CO2 equivalent did a generative model consume to be used? What is the environmental cost of sending an e-mail generated by a GenAI model? How can the carbon footprint of models be reduced?</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mstyle mathcolor="green"><mi>T</mi><mi>h</mi><mi>e</mi><mtext> </mtext><mi>m</mi><mi>a</mi><mi>i</mi><mi>n</mi><mtext> </mtext><mi>r</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>m</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mtext> </mtext><mi>i</mi><mi>s</mi><mtext> </mtext><mi>t</mi><mi>o</mi><mtext> </mtext><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi><mtext> </mtext><mi>b</mi><mi>y</mi><mtext> </mtext><mi>e</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>n</mi><mi>g</mi><mtext> </mtext><mi>t</mi><mi>h</mi><mi>e</mi><mtext> </mtext><mi>c</mi><mi>a</mi><mi>r</mi><mi>b</mi><mi>o</mi><mi>n</mi><mtext> </mtext><mi>f</mi><mi>o</mi><mi>o</mi><mi>t</mi><mi>p</mi><mi>r</mi><mi>i</mi><mi>n</mi><mi>t</mi><mtext> </mtext><mi>o</mi><mi>f</mi><mtext> </mtext><mi>A</mi><mi>I</mi><mtext> </mtext><mi>g</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mtext> </mtext><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mi>s</mi></mstyle></mrow><annotation encoding="application/x-tex">{\color{green}The \space main \space recommendation \space is \space to \space start \space by \space estimating \space the \space carbon \space footprint \space of \space AI \space generative \space models}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;color:green;">T</span><span class="mord mathnormal" style="color:green;">h</span><span class="mord mathnormal" style="color:green;">e</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">main</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">reco</span><span class="mord mathnormal" style="color:green;">mm</span><span class="mord mathnormal" style="color:green;">e</span><span class="mord mathnormal" style="color:green;">n</span><span class="mord mathnormal" style="color:green;">d</span><span class="mord mathnormal" style="color:green;">a</span><span class="mord mathnormal" style="color:green;">t</span><span class="mord mathnormal" style="color:green;">i</span><span class="mord mathnormal" style="color:green;">o</span><span class="mord mathnormal" style="color:green;">n</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">i</span><span class="mord mathnormal" style="color:green;">s</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">t</span><span class="mord mathnormal" style="color:green;">o</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">s</span><span class="mord mathnormal" style="color:green;">t</span><span class="mord mathnormal" style="color:green;">a</span><span class="mord mathnormal" style="margin-right:0.02778em;color:green;">r</span><span class="mord mathnormal" style="color:green;">t</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">b</span><span class="mord mathnormal" style="margin-right:0.03588em;color:green;">y</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">es</span><span class="mord mathnormal" style="color:green;">t</span><span class="mord mathnormal" style="color:green;">ima</span><span class="mord mathnormal" style="color:green;">t</span><span class="mord mathnormal" style="color:green;">in</span><span class="mord mathnormal" style="margin-right:0.03588em;color:green;">g</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">t</span><span class="mord mathnormal" style="color:green;">h</span><span class="mord mathnormal" style="color:green;">e</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">c</span><span class="mord mathnormal" style="color:green;">a</span><span class="mord mathnormal" style="margin-right:0.02778em;color:green;">r</span><span class="mord mathnormal" style="color:green;">b</span><span class="mord mathnormal" style="color:green;">o</span><span class="mord mathnormal" style="color:green;">n</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="margin-right:0.10764em;color:green;">f</span><span class="mord mathnormal" style="color:green;">oo</span><span class="mord mathnormal" style="color:green;">tp</span><span class="mord mathnormal" style="margin-right:0.02778em;color:green;">r</span><span class="mord mathnormal" style="color:green;">in</span><span class="mord mathnormal" style="color:green;">t</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">o</span><span class="mord mathnormal" style="margin-right:0.10764em;color:green;">f</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">A</span><span class="mord mathnormal" style="margin-right:0.07847em;color:green;">I</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="margin-right:0.03588em;color:green;">g</span><span class="mord mathnormal" style="color:green;">e</span><span class="mord mathnormal" style="color:green;">n</span><span class="mord mathnormal" style="margin-right:0.02778em;color:green;">er</span><span class="mord mathnormal" style="color:green;">a</span><span class="mord mathnormal" style="color:green;">t</span><span class="mord mathnormal" style="color:green;">i</span><span class="mord mathnormal" style="margin-right:0.03588em;color:green;">v</span><span class="mord mathnormal" style="color:green;">e</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">m</span><span class="mord mathnormal" style="color:green;">o</span><span class="mord mathnormal" style="color:green;">d</span><span class="mord mathnormal" style="color:green;">e</span><span class="mord mathnormal" style="margin-right:0.01968em;color:green;">l</span><span class="mord mathnormal" style="color:green;">s</span></span></span></span></span></span>

<p>There are no tools for estimating the total carbon footprint of the AI field. But the carbon footprint of a machine learning model can be estimated by the electricity consumption of the training procedure or inference. All software, from phone apps to cloud-based data science pipelines, consumes electricity. As not all electricity is generated from renewable sources, this consumption contributes to a carbon footprint.</p>

<p>Thanks to the Electricity Map tool <a href="https://app.electricitymaps.com/map">Link to the tool</a>, it’s possible to take action on the carbon footprint of codes. Electricity Maps is a 24/7 live visualization of where electricity comes from and how much CO2 is emitted to produce it. As a result, there’s enough information to determine in which country it’s best to locate servers.</p>

<p>Electricity production in France has very low CO2 emissions, largely thanks to its nuclear fleet and the development of renewable energies.</p>

<p>The carbon footprint of a machine learning model can be estimated by estimating the electricity consumption of the training procedure or inference. To obtain a carbon footprint estimate with this tool, simply enter the following parameters: Hardware type (GPU, CPU…), Number of hours the hardware was used, Cloud provider used, and Cloud region where the calculation took place (for example, “europe-north1”).</p>

<p>The measurement of a server’s power consumption is the sum of the power consumption of all its components. The first component is the measurement of CPU power consumption, performed by the RAPL interface on Unix operating systems or by Intel Power Gadget on Mac and Windows. The RAPL interface also provides RAM power consumption. The third component is the measurement of GPU power consumption, obtained via Nvidia SMI, which provides real-time power consumption via its interface or via Python (pyNVML).</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mstyle mathcolor="green"><mi>C</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>C</mi><mi>a</mi><mi>r</mi><mi>b</mi><mi>o</mi><mi>n</mi><mo>:</mo><mtext> </mtext><mi>t</mi><mi>h</mi><mi>e</mi><mtext> </mtext><mi>e</mi><mi>s</mi><mi>s</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mtext> </mtext><mi>t</mi><mi>o</mi><mi>o</mi><mi>l</mi><mtext> </mtext><mi>f</mi><mi>o</mi><mi>r</mi><mtext> </mtext><mi>a</mi><mi>s</mi><mi>s</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>g</mi><mtext> </mtext><mi>t</mi><mi>h</mi><mi>e</mi><mtext> </mtext><mi>e</mi><mi>n</mi><mi>v</mi><mi>i</mi><mi>r</mi><mi>o</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>a</mi><mi>l</mi><mtext> </mtext><mi>i</mi><mi>m</mi><mi>p</mi><mi>a</mi><mi>c</mi><mi>t</mi><mtext> </mtext><mi>o</mi><mi>f</mi><mtext> </mtext><mi>g</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mtext> </mtext><mi>A</mi><mi>I</mi><mtext> </mtext><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mi>s</mi></mstyle></mrow><annotation encoding="application/x-tex">{\color{green}CodeCarbon: \space the \space essential \space tool \space for \space assessing \space the \space environmental \space impact \space of \space generative \space AI \space models}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;color:green;">C</span><span class="mord mathnormal" style="color:green;">o</span><span class="mord mathnormal" style="color:green;">d</span><span class="mord mathnormal" style="color:green;">e</span><span class="mord mathnormal" style="margin-right:0.07153em;color:green;">C</span><span class="mord mathnormal" style="color:green;">a</span><span class="mord mathnormal" style="margin-right:0.02778em;color:green;">r</span><span class="mord mathnormal" style="color:green;">b</span><span class="mord mathnormal" style="color:green;">o</span><span class="mord mathnormal" style="color:green;">n</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel" style="color:green;">:</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="color:green;">t</span><span class="mord mathnormal" style="color:green;">h</span><span class="mord mathnormal" style="color:green;">e</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">esse</span><span class="mord mathnormal" style="color:green;">n</span><span class="mord mathnormal" style="color:green;">t</span><span class="mord mathnormal" style="color:green;">ia</span><span class="mord mathnormal" style="margin-right:0.01968em;color:green;">l</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">t</span><span class="mord mathnormal" style="color:green;">oo</span><span class="mord mathnormal" style="margin-right:0.01968em;color:green;">l</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="margin-right:0.10764em;color:green;">f</span><span class="mord mathnormal" style="margin-right:0.02778em;color:green;">or</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">a</span><span class="mord mathnormal" style="color:green;">ssess</span><span class="mord mathnormal" style="color:green;">in</span><span class="mord mathnormal" style="margin-right:0.03588em;color:green;">g</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">t</span><span class="mord mathnormal" style="color:green;">h</span><span class="mord mathnormal" style="color:green;">e</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">e</span><span class="mord mathnormal" style="color:green;">n</span><span class="mord mathnormal" style="margin-right:0.03588em;color:green;">v</span><span class="mord mathnormal" style="color:green;">i</span><span class="mord mathnormal" style="color:green;">ro</span><span class="mord mathnormal" style="color:green;">nm</span><span class="mord mathnormal" style="color:green;">e</span><span class="mord mathnormal" style="color:green;">n</span><span class="mord mathnormal" style="color:green;">t</span><span class="mord mathnormal" style="color:green;">a</span><span class="mord mathnormal" style="margin-right:0.01968em;color:green;">l</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">im</span><span class="mord mathnormal" style="color:green;">p</span><span class="mord mathnormal" style="color:green;">a</span><span class="mord mathnormal" style="color:green;">c</span><span class="mord mathnormal" style="color:green;">t</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">o</span><span class="mord mathnormal" style="margin-right:0.10764em;color:green;">f</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="margin-right:0.03588em;color:green;">g</span><span class="mord mathnormal" style="color:green;">e</span><span class="mord mathnormal" style="color:green;">n</span><span class="mord mathnormal" style="margin-right:0.02778em;color:green;">er</span><span class="mord mathnormal" style="color:green;">a</span><span class="mord mathnormal" style="color:green;">t</span><span class="mord mathnormal" style="color:green;">i</span><span class="mord mathnormal" style="margin-right:0.03588em;color:green;">v</span><span class="mord mathnormal" style="color:green;">e</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">A</span><span class="mord mathnormal" style="margin-right:0.07847em;color:green;">I</span><span class="mspace" style="color:green;"><span style="color:green;"> </span></span><span class="mord mathnormal" style="color:green;">m</span><span class="mord mathnormal" style="color:green;">o</span><span class="mord mathnormal" style="color:green;">d</span><span class="mord mathnormal" style="color:green;">e</span><span class="mord mathnormal" style="margin-right:0.01968em;color:green;">l</span><span class="mord mathnormal" style="color:green;">s</span></span></span></span></span></span>

<p>There’s a tool that lets you make the estimate directly by integrating it into the code. CodeCarbon is an open-source Python package designed to estimate the carbon footprint of machine learning models and other applications. It takes into account hardware energy consumption, including GPU, CPU and RAM, as well as the carbon intensity of the computational region at fixed intervals, e.g. every 15 seconds. By monitoring energy consumption and multiplying by carbon intensity, CodeCarbon provides an estimate of the carbon dioxide equivalents (CO2e) used in a single run.</p>

<p>Integrating CodeCarbon with the inference code is relatively straightforward, as this script demonstrates:</p>

<p><code style="color : Green">tracker = EmissionsTracker()</code>
<code style="color : Green">tracker.start()</code></p>

<p><code style="color : Green">#code de l’inférence API pour appeler le modèle </code>
<code style="color : Green">#pour effectuer un l'inférence</code></p>

<p><code style="color : Green">model.predict(data)</code>
<code style="color : Green">emissions: float = tracker.stop()</code></p>

<p>CodeCarbon has a built-in log that records data in a CSV file named emissions.csv in the output directory, for each experiment tracked across projects. The most important features are emissions (emissions in CO₂ equivalents [CO₂eq], in kg), emissions_rate (emissions divided by time, in kg/s), cpu_power (processor power (W)), gpu_power (GPU power (W)), ram_power (RAM power (W)), energy_consumed (sum of cpu_energy, gpu_energy and ram_energy (kWh)), Country_name (name of the country where the infrastructure is hosted), etc.</p>

<p>CodeCarbon also comes with carbonboard, which provides a dashboard for data visualization. Users can understand the net energy consumption and emissions generated across projects, and can immerse themselves in a particular experiment or project.</p>

<p><img src="/assets/img/blog/footprintcarbon.png" alt="drawing" width="800" /></p>

<p>For an example of the Llama2 generative model, the model emitted an average of 0.0002kg of CO2 with the GPU environment. By repeating the experiment 100 times to make more inferences with the same model, 0.1kg of carbon equivalent were emitted (equivalent to 31 minutes of TV or 0.03% of weekly US household emissions). To put this into perspective, 1 h of visio, for two, with the camera emits 66 g CO2e on average in France. So much for the figures! Now it’s up to you to imagine the impact of our future Generative Models on our carbon footprint. Who would have thought that our discussions could carry such weight?</p>]]></content><author><name>Pierre Samaha</name><email>pierre_samaha@hotmail.com</email></author><category term="example" /><summary type="html"><![CDATA[Start calculating the CO2 emissions of your GenAI models]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/Sustainability.jpg" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/Sustainability.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Aneurysm Segmentation</title><link href="http://localhost:4000/example/2023-11-23-Aneurysm-Segmentation/" rel="alternate" type="text/html" title="Aneurysm Segmentation" /><published>2023-11-23T00:00:00+01:00</published><updated>2024-09-28T23:07:36+02:00</updated><id>http://localhost:4000/example/Aneurysm-Segmentation</id><content type="html" xml:base="http://localhost:4000/example/2023-11-23-Aneurysm-Segmentation/"><![CDATA[<p>An intracranial aneurysm (IA) is a weakened or thinned portion of a blood vessel in the brain that bulges dangerously and fills up with blood, which can compress surrounding nerves and brain tissue and have a high risk of rupture.</p>

<p>Over the last decade, many extraction algorithms have been designed by calculating local geometric features; however, rule-based methods have high computational costs and time requirements, and their performance is limited because of the wide variety of aneurysm shapes.</p>

<p>Deep learning techniques are becoming popular in medical image processing and one proposed model takes a surface model of an entire set of principal brain arteries containing aneurysms as input and returns aneurysm surfaces as output.</p>

<p>One way to segment 3D dataset is just by looking at one 2d Slice at a time, training a 2D unet, predicting each 2d slice and combine the volume together, which works fine in most cases, but not be the ideal choice for a lot of 3D data set because we probably need information from the few planes below and above. Our proposed model takes a surface model of an entire set of principal brain arteries containing aneurysms as input and returns aneurysm surfaces as output.</p>

<h2 id="objective">Objective</h2>

<p>Precise segmentation of pre-detected aneurysms: The model we propose takes as input a surface model of a complete set of main cerebral arteries (3D MRI data) containing aneurysms and returns the aneurysm surfaces as output.</p>

<h2 id="dataset-and-challenges">Dataset and Challenges</h2>

<ol>
  <li>Working on real-life data, fresh from the hospital: 105 scans (3D MRI data)
This is one of the main challenge: the low number of training data, and the complexity and size of the data (about 3cm x 7cm x 7cm) we have.</li>
  <li>The input images are 192 by 192 by 64, we cannot load all of that into the memory.</li>
</ol>

<h2 id="preprocessing-data">Preprocessing Data</h2>

<ul>
  <li>Simple augmentation that preserves the physical aspect of the data:</li>
</ul>

<p><strong>Offline Augmentation</strong>: Offline augmentation can dramatically increase the size of the dataset: Flipping, rotation
<strong>Online Augmentation</strong>: changing the sliding axis from xyz to xzy or yzx
<img src="/assets/img/blog/onlineaug.png" alt="drawing" width="500" /></p>

<ul>
  <li>
    <p>We then divide these into patches of 64 by 64 by 64: The resolution of input 3D data is the primary reason behind the need for large memory and higher computation complexity in training a 3D CNN segmentation model. However, this could be circumvented with a patch-based approach. In patch-wise approach, the input 3D images are converted to small 3D sub-samples and analyzed individually. Finally, the patch-wise prediction outcomes will be merged to create the final 3D prediction map. The methodology uses cropped 3D patches of size (64 × 64× 64). The patch-based segmentation helps to reduce the hardware resources for training and generates a large number of data samples that favor adequate learning. However, the patch-wise analysis often fails to extract global features from the actual image volumes. This may limit the learning performance when the abnormality is region-specific.</p>
  </li>
  <li>
    <p>Randomly eliminate some patches that do not contain an aneurysm at all, To balance the number of training samples with and without aneurysms. Many of these patches remain in the data because they are considered useful for knowing when there is an aneurysm and when there is not.</p>
  </li>
  <li>
    <p>Image generator method
This amount of data and its size (3D) is not possible to hold into memory so we will be using a generator method that will yield batches. When encountering a problem where the dataset used is too big to be loaded into memory at once that is being run on RAM. For example, we won’t be able to load all those images into memory before training.So, if we create a data generator, we can read images on the go when they will be used for training. Since we are reading the images on the go, we are saving memory.</p>
  </li>
</ul>

<p>we train the model on these patches, we load the next volume , then train the model then so on.
<img src="/assets/img/blog/preprocessing.png" alt="drawing" width="500" /></p>

<h2 id="metrics-and-loss">Metrics and Loss</h2>

<p><img src="/assets/img/blog/metricsandloss.png" alt="drawing" width="500" /></p>

<h2 id="modeling">Modeling</h2>

<p>The 3D U-Net architecture is quite similar to the U-Net.
It comprises of an analysis path (left) and a synthesis path (right).
In the analysis path, each layer contains two 3×3×3 convolutions each followed by a ReLU, and then a 2×2×2 max pooling with strides of two in each dimension.
In the synthesis path, each layer consists of an up-convolution of 2×2×2 by strides of two in each dimension, followed by two 3×3×3 convolutions each followed by a ReLU.
Shortcut connections from layers of equal resolution in the analysis path provide the essential high-resolution features to the synthesis path.
In the last layer, a 1×1×1 convolution reduces the number of output channels to the number of labels which is 3.
batch normalization (\BN”) before each ReLU.
19069955 parameters in total.</p>

<p><img src="/assets/img/blog/3Dunet.png" alt="drawing" width="500" /></p>

<p>When trying with a 3D_unet from scatch, we got fluctuations in the training loss.
Transfer learning: We can define backbones (ResNet50) that have pre-trained weights for faster and better convergence for the Unet.</p>

<p><img src="/assets/img/blog/transferlearning.png" alt="drawing" width="500" /></p>

<h1 id="prediction">Prediction</h1>

<p>Instead of doing ensemble learning with different models, we have predict the 3D MRI test images with different rotation of the axis (changing the axis of the layers of the 3D MRI images), reshape them then average the transformed predictions using simple average. This give better results than predicting one version of the input test.</p>

<p><a href="https://github.com/pierresamaha1998/Data-Challenge-Aneurysm-Segmentation">Click to be directed to the code</a></p>]]></content><author><name>Pierre Samaha</name><email>pierre_samaha@hotmail.com</email></author><category term="example" /><summary type="html"><![CDATA[Healthcare and AI: Precise segmentation of 3D MRI images containing aneurysms using 3D-Unet]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/aneurysm.jpg" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/aneurysm.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>